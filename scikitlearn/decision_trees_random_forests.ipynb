{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests\n",
    "\n",
    "Previously we have looked in depth at a simple generative classifier (naive Bayes; see In Depth: Naive Bayes Classification) and a powerful discriminative classifier (support vector machines; see In-Depth: Support Vector Machines). Here we'll take a look at motivating another powerful algorithmâ€”a non-parametric algorithm called random forests. Random forests are an example of an ensemble method, meaning that it relies on aggregating the results of an ensemble of simpler estimators. The somewhat surprising result with such ensemble methods is that the sum can be greater than the parts: that is, a majority vote among a number of estimators can end up being better than any of the individual estimators doing the voting! We will see examples of this in the following sections. We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Random Forests: Decision Trees\n",
    "\n",
    "Rando mforests are an exmpale of an *ensemble learner* built on decisoin trees. For this reason we'll start by discussing decision trees themselves.\n",
    "\n",
    "Decision trees are extremely intuitive ways to classify or labels objects: you simple ask a series of questions designed to zero-in on the classification. For example, if you wanted to build a decision tree to classify an animal you come across while on a hike, you might construct the one shown here:\n",
    "\n",
    "![decision_tree](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.08-decision-tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary splitting makes this extremely efficient: in a well-constructed tree, each question will cut hte number of options by approximately half, very quickly narrowing hte options even among a large number of classes. The trick, of course, comes in deciding which question to ask at each step. In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data: that is, each node in the tree splits the dat ainto two groups usinga cutoff value within one of the features. Let's now look at an example of this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6  ('.env': venv)",
   "name": "pythonjvsc74a57bd06b3d8fded84b82a84dd06aec3772984b6fd7683755c4932dae62599619bfeba9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "6b3d8fded84b82a84dd06aec3772984b6fd7683755c4932dae62599619bfeba9"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}