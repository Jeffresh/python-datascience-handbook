{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('.env')",
   "metadata": {
    "interpreter": {
     "hash": "6b3d8fded84b82a84dd06aec3772984b6fd7683755c4932dae62599619bfeba9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Feature Enginering \n",
    "\n",
    "The previous sections outline the fundamental ideas of machine learning, but all fo the exmaples assume that you have numerical data in a tidy, `[n_samples, n_features]` format. In the real world, data rarely comes in such a form. With this in mind, one of the more important steps in using machine learning in practice is *feature engineering*: that is, taking whatever information you have about your problem and turning it into numbers that you can use to build your feature matrix.\n",
    "\n",
    "In this section, we will cover a few common examples of feature engineering tasks:\n",
    "\n",
    "features  for representing *categorical data*, features for representing *text*, and *features* for representing *images*. Additionally, we will discuss *derived features* for increasing model complexity and *imputation* of missin data. Often this process is known as *vectorization*, as it involves converting arbitratry data into well-behaved vectors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Categorical Features \n",
    "\n",
    "One common type of non-numerical data is *categorical* data. For example, imagine you are exploring some data on housing prices, and along with numerical features like \"price\" and \"rooms\", you also have \"neighborhood\" information. For example, your data might look something like this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n",
    "    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\n",
    "    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\n",
    "    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\n",
    "]"
   ]
  },
  {
   "source": [
    "You might be tempted to encode this data with a straightforward numerical mapping:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "{'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3}"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3}"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "source": [
    "It turns out that his is not generally a useful approach in Scikit-Learn: the package's models make the fundamental assumption that numerical features reflect algebraic quantities. Thus such a mapping would imply, for example, that *Queen Anne < Fremont < Wallingford* or even that *Wallingford - Queen Anne* = *Fremont*, which (niche demographic jokes aside) does not make such sense.\n",
    "\n",
    "In this case, one proven technique is to use *one-hot encoding*, which effectively creates extra columns indicating the presence or absence of a category with a value or 1 or 0, respectively. When your data comes as list of dictionaries, Scikit-Learn's `DictVectorizer` will do this for you:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[     0,      1,      0, 850000,      4],\n",
       "       [     1,      0,      0, 700000,      3],\n",
       "       [     0,      0,      1, 650000,      3],\n",
       "       [     1,      0,      0, 600000,      2]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer(sparse=False, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "source": [
    "Notice that the 'neighborhood' column has been expanded into three separate columns, representing the three neighborhod labels, and that each row has a 1 in the column associated with its neighborhood. With these categorical features thus encodes, you can proceed as normal with fitting a Scikit-Learn model.\n",
    "\n",
    "To see the meaning of each column, you can inspect the feature names:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['neighborhood=Fremont',\n",
       " 'neighborhood=Queen Anne',\n",
       " 'neighborhood=Wallingford',\n",
       " 'price',\n",
       " 'rooms']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "source": [
    "There is one clear disadvantage of this approach: if your category has many possible values, this can *greatly* increase the size of your dataset. However, because the encodes data contains mostly zeros, a sparse output can be a very efficient solution:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<4x5 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "vec = DictVectorizer(sparse=True, dtype=int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "source": [
    "Many (though not yer all) of the Scikit-Learn estimators accept such sparse inputs when fitting and evaluating models. `sklearn.preprocessing.OneHotEncoder` and `sklearn.feature-extraction.FeatureHasher` are two additional tools taht Scikiti-Learn includes to support this type of encoding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}